name: Google Maps Scraper

on:
  workflow_dispatch:
    inputs:
      action:
        description: 'Action to perform'
        required: true
        default: 'start_scraper'
      timestamp:
        description: 'Timestamp of request'
        required: true

jobs:
  scrape-google-maps:
    runs-on: ubuntu-latest
    timeout-minutes: 360  # 6 hours max runtime

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install playwright pandas python-dotenv supabase

      - name: Install Playwright browsers
        run: |
          playwright install chromium
          playwright install-deps chromium

      - name: Create scraper script
        run: |
          """
          Google Maps Scraper for Ultimate Lead Gen - IMPROVED VERSION

          This script:
          1. Fetches all unused searches from Supabase "Google_Maps Searches" table
          2. Uses headless Playwright browser to scrape Google Maps for each search
          3. Extracts business data using reliable selectors (title, address, phone, rating, website, etc.)
          4. Stores results in Supabase "Roofing Leads New" table
          5. Updates searchUSED flag to true for processed searches
          6. Handles Google Maps DOM structure changes gracefully
          """

          import asyncio
          import os
          from playwright.async_api import async_playwright
          from supabase import create_client, Client
          import json
          from datetime import datetime
          import re

          # Initialize Supabase client
          SUPABASE_URL = os.getenv('SUPABASE_URL')
          SUPABASE_KEY = os.getenv('SUPABASE_SERVICE_ROLE_KEY')

          supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

          async def scrape_google_maps_search(page, search_query: str) -> list:
              """
              Scrape Google Maps for a specific search query
              Returns list of business data dictionaries
              
              Uses multiple selector strategies to handle Google Maps DOM variations
              """
              businesses = []
              
              try:
                  # Navigate to Google Maps with search query
                  search_url = f"https://www.google.com/maps/search/{search_query.replace(' ', '+')}"
                  
                  print(f"  Navigating to: {search_url}")
                  
                  # Use domcontentloaded instead of networkidle (faster, more reliable for Google Maps)
                  # Increase timeout to 60 seconds to handle slow connections
                  await page.goto(search_url, wait_until='domcontentloaded', timeout=60000)
                  
                  # Wait for results to load (reduced from 3000ms to 1000ms)
                  await page.wait_for_timeout(1000)
                  
                  # Strategy 1: Try to find business cards using the primary selector
                  # div.Nv2PK.THOPZb.CpccDe is the main business card container in Google Maps
                  business_elements = await page.query_selector_all('div.Nv2PK.THOPZb.CpccDe')
                  
                  print(f"  Found {len(business_elements)} business cards (primary selector)")
                  
                  # Strategy 2: If primary selector fails, try alternative selectors
                  if not business_elements:
                      # Try data-item-id selector (older/alternative structure)
                      business_elements = await page.query_selector_all('[data-item-id]')
                      print(f"  Found {len(business_elements)} business cards (data-item-id selector)")
                  
                  # Strategy 3: If still no results, try role-based selector
                  if not business_elements:
                      business_elements = await page.query_selector_all('div[role="button"][jsaction*="click"]')
                      print(f"  Found {len(business_elements)} business cards (role-based selector)")
                  
                  # Process each business card
                  for idx, element in enumerate(business_elements[:20]):  # Limit to first 20 results per search
                      try:
                          # Click on business to open details panel
                          await element.click()
                          await page.wait_for_timeout(500)  # Wait for details to load
                          
                          # Extract business information from the details panel
                          business_data = await extract_business_details(page)
                          
                          # Only add if we successfully extracted a title (minimum requirement)
                          if business_data and business_data.get('title'):
                              businesses.append(business_data)
                              print(f"    ✓ [{idx+1}] {business_data.get('title')}")
                          else:
                              print(f"    ✗ [{idx+1}] Failed to extract business data")
                              
                      except Exception as e:
                          # Silently skip businesses that fail to extract
                          print(f"    ✗ [{idx+1}] Error: {str(e)[:50]}")
                          continue
                  
              except asyncio.TimeoutError:
                  print(f"  ⚠ Timeout: Page took too long to load for '{search_query}'")
              except Exception as e:
                  print(f"  ⚠ Error scraping '{search_query}': {str(e)[:100]}")
              
              return businesses

          async def extract_business_details(page) -> dict:
              """
              Extract detailed business information from Google Maps listing details panel
              
              Tries multiple selectors for each field to handle DOM variations
              """
              try:
                  business_data = {
                      'title': None,
                      'address': None,
                      'phone_number': None,
                      'rating': None,
                      'webpage': None,
                      'category': None,
                      'working_hours': None,
                      'map_link': page.url,
                      'cover_image': None,
                  }
                  
                  # Extract title - try multiple selectors
                  # Primary: h1 in details panel
                  title = None
                  title_element = await page.query_selector('h1')
                  if title_element:
                      title = await title_element.text_content()
                      title = title.strip() if title else None
                  
                  # Fallback: Try .qBF1Pd selector (business name in card)
                  if not title:
                      title_element = await page.query_selector('.qBF1Pd')
                      if title_element:
                          title = await title_element.text_content()
                          title = title.strip() if title else None
                  
                  business_data['title'] = title
                  
                  # Extract rating - look for aria-label with "stars"
                  # Rating is typically in format "4.5 stars (123 reviews)"
                  try:
                      rating_element = await page.query_selector('span[aria-label*="star"]')
                      if rating_element:
                          rating_text = await rating_element.get_attribute('aria-label')
                          if rating_text:
                              # Extract numeric rating using regex (e.g., "4.5" from "4.5 stars")
                              match = re.search(r'(\d+\.?\d*)\s*star', rating_text)
                              if match:
                                  business_data['rating'] = match.group(1)
                  except:
                      pass
                  
                  # Extract address - look for address-related text
                  try:
                      # Try to find address in the details panel
                      address_elements = await page.query_selector_all('div[data-item-id] span')
                      for elem in address_elements:
                          text = await elem.text_content()
                          if text and any(keyword in text.lower() for keyword in ['street', 'ave', 'rd', 'blvd', 'ln', 'ct', 'dr']):
                              business_data['address'] = text.strip()
                              break
                  except:
                      pass
                  
                  # Extract phone number - look for phone icon or tel: links
                  try:
                      # Try to find phone number in links
                      phone_link = await page.query_selector('a[href^="tel:"]')
                      if phone_link:
                          href = await phone_link.get_attribute('href')
                          if href:
                              # Extract phone from tel: link
                              phone = href.replace('tel:', '').strip()
                              business_data['phone_number'] = phone
                      
                      # Fallback: Look for phone text in the page
                      if not business_data['phone_number']:
                          page_text = await page.text_content()
                          # Simple regex to find phone numbers (basic pattern)
                          phone_match = re.search(r'\+?1?\s*\(?(\d{3})\)?[\s.-]?(\d{3})[\s.-]?(\d{4})', page_text)
                          if phone_match:
                              business_data['phone_number'] = phone_match.group(0).strip()
                  except:
                      pass
                  
                  # Extract website - look for website links
                  try:
                      # Try to find website link
                      website_link = await page.query_selector('a[href*="http"][href*="://"]')
                      if website_link:
                          href = await website_link.get_attribute('href')
                          if href and not 'google.com' in href and not 'maps.google' in href:
                              business_data['webpage'] = href
                  except:
                      pass
                  
                  # Extract working hours - look for hours information
                  try:
                      hours_text = await page.text_content('div[aria-label*="hour"]')
                      if hours_text:
                          business_data['working_hours'] = hours_text.strip()
                  except:
                      pass
                  
                  # Extract category - look for business type/category
                  try:
                      # Category is often shown near the title
                      category_element = await page.query_selector('span.DkEaL')
                      if category_element:
                          category = await category_element.text_content()
                          business_data['category'] = category.strip() if category else None
                  except:
                      pass
                  
                  # Extract cover image - look for business image
                  try:
                      img_element = await page.query_selector('img[alt*="Photo"]')
                      if img_element:
                          src = await img_element.get_attribute('src')
                          if src:
                              business_data['cover_image'] = src
                  except:
                      pass
                  
                  # Return business data if we have at least a title
                  if business_data['title']:
                      return business_data
                  
                  return None
                  
              except Exception as e:
                  print(f"    Error extracting details: {str(e)[:50]}")
                  return None

          async def fetch_unused_searches() -> list:
              """
              Fetch all unused searches from Supabase
              Queries the "Google_Maps Searches" table for entries where searchUSED = false
              """
              try:
                  response = supabase.table('Google_Maps Searches').select('*').eq('searchUSED', False).execute()
                  return response.data if response.data else []
              except Exception as e:
                  print(f"Error fetching searches: {e}")
                  return []

          async def store_business_in_database(business_data: dict) -> bool:
              """
              Store business data in Supabase "Roofing Leads New" table
              Handles duplicate entries gracefully
              """
              try:
                  # Get next row number
                  response = supabase.table('Roofing Leads New').select('*').order('RowNumber', desc=True).limit(1).execute()
                  next_row = (response.data[0]['RowNumber'] + 1) if response.data else 1
                  
                  # Prepare data for insertion
                  data = {
                      'RowNumber': next_row,
                      'title': business_data.get('title'),
                      'address': business_data.get('address'),
                      'phone_number': business_data.get('phone_number'),
                      'rating': business_data.get('rating'),
                      'webpage': business_data.get('webpage'),
                      'category': business_data.get('category'),
                      'working_hours': business_data.get('working_hours'),
                      'map_link': business_data.get('map_link'),
                      'cover_image': business_data.get('cover_image'),
                      'Used': False,
                  }
                  
                  # Insert with duplicate handling
                  supabase.table('Roofing Leads New').insert(data).execute()
                  return True
              except Exception as e:
                  # Duplicate entry - skip silently
                  if 'duplicate' in str(e).lower():
                      return True
                  print(f"    Database error: {str(e)[:50]}")
                  return False

          async def mark_search_as_used(search_query: str) -> bool:
              """
              Mark search as used in Supabase
              Updates the searchUSED flag to true for the given search
              """
              try:
                  supabase.table('Google_Maps Searches').update({'searchUSED': True}).eq('Searches', search_query).execute()
                  return True
              except Exception as e:
                  print(f"Error marking search as used: {e}")
                  return False

          async def main():
              """
              Main scraper function
              Orchestrates the entire scraping workflow
              """
              print("=" * 60)
              print("Starting Google Maps Scraper")
              print("=" * 60)
              
              # Fetch unused searches
              searches = await fetch_unused_searches()
              print(f"\nFound {len(searches)} unused searches to process\n")
              
              total_businesses = 0
              searches_processed = 0
              results = {
                  "total_businesses": 0,
                  "searches_processed": 0,
                  "timestamp": datetime.now().isoformat(),
                  "status": "running"
              }
              
              try:
                  async with async_playwright() as p:
                      # Launch browser with stealth mode to avoid detection
                      browser = await p.chromium.launch(headless=True)
                      context = await browser.new_context(
                          user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
                      )
                      page = await context.new_page()
                      
                      # Process each search
                      for idx, search in enumerate(searches):
                          search_query = search['Searches']
                          print(f"[{idx+1}/{len(searches)}] Processing: '{search_query}'")
                          
                          try:
                              # Scrape businesses for this search
                              businesses = await scrape_google_maps_search(page, search_query)
                              print(f"  → Extracted {len(businesses)} businesses")
                              
                              # Store each business in database
                              stored_count = 0
                              for business in businesses:
                                  if await store_business_in_database(business):
                                      total_businesses += 1
                                      stored_count += 1
                              
                              print(f"  → Stored {stored_count} businesses in database")
                              
                              # Mark search as used
                              await mark_search_as_used(search_query)
                              searches_processed += 1
                              
                          except Exception as e:
                              print(f"  ✗ Error processing search: {str(e)[:100]}")
                              continue
                          
                          print()  # Blank line for readability
                      
                      await browser.close()
              
              except Exception as e:
                  print(f"Browser error: {e}")
              
              # Update results
              results["total_businesses"] = total_businesses
              results["searches_processed"] = searches_processed
              results["status"] = "completed"
              
              print("=" * 60)
              print(f"Scraping Complete!")
              print(f"  • Searches processed: {searches_processed}/{len(searches)}")
              print(f"  • Total businesses extracted: {total_businesses}")
              print(f"  • Timestamp: {results['timestamp']}")
              print("=" * 60)
              
              # Save results to file for artifact upload
              with open("scraper_results.json", "w") as f:
                  json.dump(results, f, indent=2)
              
              print("\n✓ Results file created: scraper_results.json")
              return total_businesses

          # Run the scraper
          if __name__ == '__main__':
              try:
                  asyncio.run(main())
              except Exception as e:
                  print(f"Fatal error: {e}")
              finally:
                  # Always ensure results file exists
                  if not os.path.exists("scraper_results.json"):
                      results = {
                          "total_businesses": 0,
                          "searches_processed": 0,
                          "timestamp": datetime.now().isoformat(),
                          "status": "completed_with_errors"
                      }
                      with open("scraper_results.json", "w") as f:
                          json.dump(results, f, indent=2)
                      print("✓ Created fallback results file")

      - name: Run scraper
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
        run: |
          python scraper.py

      - name: Upload results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: scraper-results
          path: scraper_results.json
          retention-days: 30

  notify-completion:
    needs: scrape-google-maps
    runs-on: ubuntu-latest
    if: always()
    steps:
      - name: Notify completion
        run: |
          echo "Scraper workflow completed"
          echo "Status: ${{ needs.scrape-google-maps.result }}"
