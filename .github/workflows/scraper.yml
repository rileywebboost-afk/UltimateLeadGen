name: Google Maps Scraper

on:
  workflow_dispatch:
    inputs:
      action:
        description: 'Action to perform'
        required: true
        default: 'start_scraper'
      timestamp:
        description: 'Timestamp of request'
        required: true

jobs:
  scrape-google-maps:
    runs-on: ubuntu-latest
    timeout-minutes: 360  # 6 hours max runtime

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install playwright pandas python-dotenv supabase

      - name: Install Playwright browsers
        run: |
          playwright install chromium
          playwright install-deps chromium

      - name: Create scraper script
        run: |
          cat > scraper.py << 'SCRAPER_EOF'
          """
          Google Maps Scraper for Ultimate Lead Gen
          
          This script:
          1. Fetches all unused searches from Supabase "Google_Maps Searches" table
          2. Uses headless Playwright browser to scrape Google Maps for each search
          3. Extracts business data (title, address, phone, rating, website, etc.)
          4. Stores results in Supabase "Roofing Leads New" table
          5. Updates searchUSED flag to true for processed searches
          """
          
          import asyncio
          import os
          from playwright.async_api import async_playwright
          from supabase import create_client, Client
          import json
          from datetime import datetime
          
          # Initialize Supabase client
          SUPABASE_URL = os.getenv('SUPABASE_URL')
          SUPABASE_KEY = os.getenv('SUPABASE_SERVICE_ROLE_KEY')
          
          supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)
          
          async def scrape_google_maps_search(page, search_query: str) -> list:
              """
              Scrape Google Maps for a specific search query
              Returns list of business data dictionaries
              """
              businesses = []
              
              try:
                  # Navigate to Google Maps with search query
                  search_url = f"https://www.google.com/maps/search/{search_query.replace(' ', '+')}"
                  
                  # Use domcontentloaded instead of networkidle (faster, more reliable for Google Maps)
                  # Increase timeout to 60 seconds to handle slow connections
                  await page.goto(search_url, wait_until='domcontentloaded', timeout=60000)
                  
                  # Wait for results to load (reduced from 3000ms to 1000ms)
                  await page.wait_for_timeout(1000)
                  
                  # Extract business listings using multiple selector strategies
                  # Try different selectors as Google Maps structure can vary
                  business_elements = await page.query_selector_all('[data-item-id]')
                  
                  # If no elements found with data-item-id, try alternative selectors
                  if not business_elements:
                      business_elements = await page.query_selector_all('div[role="button"][jsaction*="click"]')
                  
                  print(f"Found {len(business_elements)} business elements")
                  
                  for idx, element in enumerate(business_elements[:20]):  # Limit to first 20 results per search
                      try:
                          # Click on business to open details panel
                          await element.click()
                          await page.wait_for_timeout(500)  # Reduced wait time
                          
                          # Extract business information
                          business_data = await extract_business_details(page)
                          
                          if business_data and business_data.get('title'):
                              businesses.append(business_data)
                              print(f"  âœ“ Extracted: {business_data.get('title')}")
                      except Exception as e:
                          # Silently skip businesses that fail to extract
                          continue
                  
              except asyncio.TimeoutError:
                  print(f"Timeout scraping search '{search_query}': Page took too long to load")
              except Exception as e:
                  print(f"Error scraping search '{search_query}': {str(e)[:100]}")
              
              return businesses
          
          async def extract_business_details(page) -> dict:
              """
              Extract detailed business information from Google Maps listing
              """
              try:
                  # Extract title - try multiple selectors
                  title = None
                  title_element = await page.query_selector('h1')
                  if title_element:
                      title = await title_element.text_content()
                      title = title.strip() if title else None
                  
                  # Extract rating
                  rating = None
                  try:
                      rating_text = await page.text_content('[aria-label*="star"]')
                      rating = rating_text.strip() if rating_text else None
                  except:
                      pass
                  
                  # Extract address
                  address = None
                  try:
                      address_text = await page.text_content('[data-item-id] [aria-label*="address"]')
                      address = address_text.strip() if address_text else None
                  except:
                      pass
                  
                  # Extract phone
                  phone_number = None
                  try:
                      phone_text = await page.text_content('[data-item-id] [aria-label*="phone"]')
                      phone_number = phone_text.strip() if phone_text else None
                  except:
                      pass
                  
                  # Extract website
                  webpage = None
                  try:
                      website_element = await page.query_selector('[data-item-id] a[href*="http"]')
                      if website_element:
                          webpage = await website_element.get_attribute('href')
                  except:
                      pass
                  
                  # Extract map link
                  map_link = page.url
                  
                  # Return business data if we have at least a title
                  if title:
                      return {
                          'title': title,
                          'address': address,
                          'phone_number': phone_number,
                          'rating': rating,
                          'webpage': webpage,
                          'category': None,
                          'working_hours': None,
                          'map_link': map_link,
                          'cover_image': None,
                      }
                  
                  return None
              except Exception as e:
                  return None
          
          async def fetch_unused_searches() -> list:
              """
              Fetch all unused searches from Supabase
              """
              try:
                  response = supabase.table('Google_Maps Searches').select('*').eq('searchUSED', False).execute()
                  return response.data if response.data else []
              except Exception as e:
                  print(f"Error fetching searches: {e}")
                  return []
          
          async def store_business_in_database(business_data: dict) -> bool:
              """
              Store business data in Supabase "Roofing Leads New" table
              """
              try:
                  # Get next row number
                  response = supabase.table('Roofing Leads New').select('*').order('RowNumber', desc=True).limit(1).execute()
                  next_row = (response.data[0]['RowNumber'] + 1) if response.data else 1
                  
                  # Prepare data for insertion
                  data = {
                      'RowNumber': next_row,
                      'title': business_data.get('title'),
                      'address': business_data.get('address'),
                      'phone_number': business_data.get('phone_number'),
                      'rating': business_data.get('rating'),
                      'webpage': business_data.get('webpage'),
                      'category': business_data.get('category'),
                      'working_hours': business_data.get('working_hours'),
                      'map_link': business_data.get('map_link'),
                      'cover_image': business_data.get('cover_image'),
                      'Used': False,
                  }
                  
                  # Insert with duplicate handling
                  supabase.table('Roofing Leads New').insert(data).execute()
                  return True
              except Exception as e:
                  # Duplicate entry - skip silently
                  if 'duplicate' in str(e).lower():
                      return True
                  return False
          
          async def mark_search_as_used(search_query: str) -> bool:
              """
              Mark search as used in Supabase
              """
              try:
                  supabase.table('Google_Maps Searches').update({'searchUSED': True}).eq('Searches', search_query).execute()
                  return True
              except Exception as e:
                  print(f"Error marking search as used: {e}")
                  return False
          
          async def main():
              """
              Main scraper function
              """
              print("Starting Google Maps scraper...")
              
              # Fetch unused searches
              searches = await fetch_unused_searches()
              print(f"Found {len(searches)} unused searches")
              
              total_businesses = 0
              results = {
                  "total_businesses": 0,
                  "searches_processed": 0,
                  "timestamp": datetime.now().isoformat(),
                  "status": "running"
              }
              
              try:
                  async with async_playwright() as p:
                      # Launch browser with stealth mode
                      browser = await p.chromium.launch(headless=True)
                      context = await browser.new_context(
                          user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                      )
                      page = await context.new_page()
                      
                      # Process each search
                      for idx, search in enumerate(searches):
                          search_query = search['Searches']
                          print(f"[{idx+1}/{len(searches)}] Scraping: {search_query}")
                          
                          try:
                              # Scrape businesses for this search
                              businesses = await scrape_google_maps_search(page, search_query)
                              print(f"Found {len(businesses)} businesses")
                              
                              # Store each business in database
                              for business in businesses:
                                  if await store_business_in_database(business):
                                      total_businesses += 1
                              
                              # Mark search as used
                              await mark_search_as_used(search_query)
                              results["searches_processed"] += 1
                              
                          except Exception as e:
                              print(f"Error processing search '{search_query}': {str(e)[:100]}")
                              continue
                      
                      await browser.close()
              
              except Exception as e:
                  print(f"Browser error: {e}")
              
              # Update results
              results["total_businesses"] = total_businesses
              results["status"] = "completed"
              
              print(f"\nScraping complete! Total businesses stored: {total_businesses}")
              
              # Save results to file for artifact upload
              with open("scraper_results.json", "w") as f:
                  json.dump(results, f, indent=2)
              
              print("Results file created: scraper_results.json")
              return total_businesses
          
          # Run the scraper
          if __name__ == '__main__':
              try:
                  asyncio.run(main())
              except Exception as e:
                  print(f"Fatal error: {e}")
              finally:
                  # Always ensure results file exists
                  if not os.path.exists("scraper_results.json"):
                      results = {
                          "total_businesses": 0,
                          "searches_processed": 0,
                          "timestamp": datetime.now().isoformat(),
                          "status": "completed_with_errors"
                      }
                      with open("scraper_results.json", "w") as f:
                          json.dump(results, f, indent=2)
                      print("Created fallback results file")
          SCRAPER_EOF

      - name: Run scraper
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
        run: |
          python scraper.py

      - name: Upload results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: scraper-results
          path: scraper_results.json
          retention-days: 30

  notify-completion:
    needs: scrape-google-maps
    runs-on: ubuntu-latest
    if: always()
    steps:
      - name: Notify completion
        run: |
          echo "Scraper workflow completed"
          echo "Status: ${{ needs.scrape-google-maps.result }}"
